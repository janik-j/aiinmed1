{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjehk\\miniconda3\\envs\\SDEdit\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import medmnist\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, convert_to_binary, epochs, device):\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        for inputs, targets in tqdm(loader):\n",
    "            # forward + backward + optimize\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            targets = targets.squeeze().long()\n",
    "\n",
    "            # TODO: Convert to binary classification\n",
    "            # check if at least one disease label is present and convert to True/False\n",
    "            if convert_to_binary:\n",
    "                targets = torch.where(targets.sum(dim=1) > 0, torch.tensor([1]).to(device), torch.tensor([0]).to(device)).long()\n",
    "                \n",
    "            loss = criterion_ce(outputs, targets)\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss /= len(loader)\n",
    "        print('Epoch: {}\\tLoss: {:.4f}'.format(epoch+1, avg_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device, convert_to_binary):\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    y_true = torch.tensor([], device=device)\n",
    "    y_score = torch.tensor([], device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            targets = targets.squeeze().long()\n",
    "\n",
    "            # TODO: Convert to binary classification\n",
    "            if convert_to_binary:\n",
    "                targets = torch.where(targets.sum(dim=1) > 0, torch.tensor([1]).to(device), torch.tensor([0]).to(device)).long()\n",
    "                \n",
    "            loss = criterion_ce(outputs, targets)\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_preds = torch.max(y_score, dim=1)[1]\n",
    "        TP = ((y_preds == 1) & (y_true == 1)).sum().item()\n",
    "        TN = ((y_preds == 0) & (y_true == 0)).sum().item()\n",
    "        FP = ((y_preds == 1) & (y_true == 0)).sum().item()\n",
    "        FN = ((y_preds == 0) & (y_true == 1)).sum().item()\n",
    "\n",
    "        # TODO: Accuracy\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "        print('acc:%.3f' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag_chest = 'chestmnist'\n",
    "download = True\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "info_chest = INFO[data_flag_chest]\n",
    "task_chest = info_chest['task']\n",
    "n_channels_chest = info_chest['n_channels']\n",
    "#n_classes_chest = len(info_chest['label'])\n",
    "n_classes_chest = 2\n",
    "DataClass_chest = getattr(medmnist, info_chest['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\jjehk\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\jjehk\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\jjehk\\.medmnist\\chestmnist.npz\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the chest data\n",
    "train_dataset_chest = DataClass_chest(split='train', transform=data_transform, download=download)\n",
    "test_dataset_chest = DataClass_chest(split='test', transform=data_transform, download=download)\n",
    "\n",
    "train_dataset_chest_full = DataClass_chest(split='train', transform=data_transform, download=download)\n",
    "train_loader_chest_full = data.DataLoader(dataset=train_dataset_chest_full, batch_size=256, shuffle=True)\n",
    "\n",
    "# Lets pretend we only have labels for 300 training examples\n",
    "train_dataset_chest.imgs = train_dataset_chest.imgs[:300]\n",
    "train_dataset_chest.labels = train_dataset_chest.labels[:300]\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader_chest = data.DataLoader(dataset=train_dataset_chest, batch_size=64, shuffle=True)\n",
    "test_loader_chest = data.DataLoader(dataset=test_dataset_chest, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 49.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 0.7314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 47.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\tLoss: 0.6157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 57.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\tLoss: 0.5437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 51.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\tLoss: 0.4858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 58.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\tLoss: 0.4250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 51.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\tLoss: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 43.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\tLoss: 0.3446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 56.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\tLoss: 0.3082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 52.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\tLoss: 0.2771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 56.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\tLoss: 0.2569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 54.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\tLoss: 0.2215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 53.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\tLoss: 0.2077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 52.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\tLoss: 0.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 59.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\tLoss: 0.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 52.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\tLoss: 0.1504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 56.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\tLoss: 0.1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 52.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\tLoss: 0.1211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 54.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\tLoss: 0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 52.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\tLoss: 0.0972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 55.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\tLoss: 0.0910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define a simple CNN model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*4, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.extract_embeddings = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "            \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_chest = Net(in_channels=n_channels_chest, num_classes=2).to(device)\n",
    "\n",
    "optimizer_chest = optim.SGD(model_chest.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "model_chest = train(model_chest, train_loader_chest, optimizer_chest, convert_to_binary=True, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.580\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jjehk\\Downloads\\transferlearning.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jjehk/Downloads/transferlearning.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Examine the data distribution of the binary classification task. Is accuracy a good metric?\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jjehk/Downloads/transferlearning.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test(model_chest, test_loader_chest, device, convert_to_binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jjehk/Downloads/transferlearning.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m positive_rate \u001b[39m=\u001b[39m TP \u001b[39m/\u001b[39m (TP \u001b[39m+\u001b[39m FN)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jjehk/Downloads/transferlearning.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPositive rate: \u001b[39m\u001b[39m\"\u001b[39m, positive_rate)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TP' is not defined"
     ]
    }
   ],
   "source": [
    "# Examine the data distribution of the binary classification task. Is accuracy a good metric?\n",
    "#positive_rate = TP / (TP + FN)\n",
    "print(\"Positive rate: \", positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Evaluating ...\n",
      "train\n",
      "acc:0.997\n",
      "test\n",
      "acc:0.580\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "print('==> Evaluating ...')\n",
    "print('train')\n",
    "test(model_chest, train_loader_chest, device, convert_to_binary=True)\n",
    "print('test')\n",
    "test(model_chest, test_loader_chest, device, convert_to_binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMINE LATENT SPACE\n",
    "\n",
    "# Function to extract embeddings - TODO: Adjust the Net class to extract embeddings\n",
    "def extract_embeddings(dataloader, model):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, target in dataloader:\n",
    "            x = x.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            model.extract_embeddings = True\n",
    "            output = model(x)\n",
    "            model.extract_embeddings = False\n",
    "            \n",
    "            embeddings.append(output)\n",
    "            labels.append(target)\n",
    "    \n",
    "    return torch.cat(embeddings), torch.cat(labels)\n",
    "\n",
    "# Function to plot\n",
    "def plot_embeddings(embeddings, labels, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.show()\n",
    "\n",
    "test_embeddings, test_labels = extract_embeddings(test_loader_chest, model_chest)\n",
    "\n",
    "# TODO: convert labels to binary\n",
    "test_labels =\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten the embeddings\n",
    "test_embeddings_flat = test_embeddings.view(test_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "# TODO: Apply PCA\n",
    "\n",
    "# TODO: Apply t-SNE \n",
    "\n",
    "# Plot PCA result\n",
    "plot_embeddings(pca_result, test_labels.cpu(), 'PCA of Train Embeddings')\n",
    "\n",
    "# Plot t-SNE result\n",
    "plot_embeddings(tsne_result, test_labels.cpu(), 't-SNE of Train Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TODO: Define an autoencoder model that is compatable with the previous model. Use dropout with probability p\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, p):\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model_ae = Autoencoder(p=0.05).to(device)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.AdamW(model_ae.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "# Metrics\n",
    "psnr_metric = PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_ae.train()\n",
    "    for x in train_loader_chest_full:\n",
    "        inputs, _ = x\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_ae(inputs)\n",
    "        loss = criterion_ae(outputs, inputs)\n",
    "\n",
    "        psnr_value = psnr_metric(outputs, inputs)\n",
    "        ssim_value = ssim_metric(outputs, inputs)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer_ae.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ae.step()\n",
    "\n",
    "    # Print the metrics for every epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, PSNR: {psnr_value.item():.4f}, SSIM: {ssim_value.item():.4f}')\n",
    "\n",
    "    psnr_metric.reset()\n",
    "    ssim_metric.reset()\n",
    "\n",
    "# Test the autoencoder on the test set\n",
    "model_ae.eval()\n",
    "test_loss = 0.0\n",
    "psnr_value_test = 0.0, 0.0\n",
    "ssim_value_test = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for x in test_loader_chest:\n",
    "        inputs, _ = x\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_ae(inputs)\n",
    "\n",
    "        loss = criterion_ae(outputs, inputs)\n",
    "        psnr_value_test = psnr_metric(outputs, inputs)\n",
    "        ssim_value_test = ssim_metric(outputs, inputs)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "average_test_loss = test_loss / len(test_loader_chest.dataset)\n",
    "average_psnr_test = psnr_metric.compute()\n",
    "average_ssim_test = ssim_metric.compute()\n",
    "\n",
    "print(f'Average Test Loss: {average_test_loss:.4f}, Average PSNR: {average_psnr_test:.4f}, Average SSIM: {average_ssim_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# TODO: Function to plot input and output images\n",
    "def plot_images(model, test_loader, device, num_images=5):\n",
    "\n",
    "# Use the function to plot images\n",
    "plot_images(model_ae, test_loader_chest, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with randomly initialized weights\n",
    "model_transfer = Net(in_channels=n_channels_chest, num_classes=2).to(device)\n",
    "\n",
    "# TODO: Transfer the weights into the new model\n",
    "\n",
    "# train\n",
    "optimizer_transfer = optim.SGD(model_transfer.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "model_transfer = train(model_transfer, train_loader_chest, optimizer_transfer, convert_to_binary=True, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> Evaluating ...')\n",
    "test(model_transfer, train_loader_chest, device, convert_to_binary=True)\n",
    "test(model_transfer, test_loader_chest, device, convert_to_binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag_pneumonia = 'pneumoniamnist'\n",
    "info_pneumonia = INFO[data_flag_pneumonia]\n",
    "task_pneumonia = info_pneumonia['task']\n",
    "n_channels_pneumonia = info_pneumonia['n_channels']\n",
    "n_classes_pneumonia = len(info_pneumonia['label'])\n",
    "DataClass_pneumonia = getattr(medmnist, info_pneumonia['python_class'])\n",
    "\n",
    "# load the pneumonia data\n",
    "train_dataset_pneumonia = DataClass_pneumonia(split='train', transform=data_transform, download=download)\n",
    "test_dataset_pneumonia = DataClass_pneumonia(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset_pneumonia = DataClass_pneumonia(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader_pneumonia = data.DataLoader(dataset=train_dataset_pneumonia, batch_size=64, shuffle=True)\n",
    "test_loader_pneumonia = data.DataLoader(dataset=test_dataset_pneumonia, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pneumonia = Net(in_channels=n_channels_pneumonia, num_classes=2).to(device)\n",
    "\n",
    "optimizer_pneumonia = optim.SGD(model_pneumonia.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "model_pneumonia = train(model_pneumonia, train_loader_pneumonia, optimizer_pneumonia, convert_to_binary=False, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "print('==> Evaluating ...')\n",
    "print('train')\n",
    "test(model_pneumonia, train_loader_pneumonia, device, convert_to_binary=False)\n",
    "print('test')\n",
    "test(model_pneumonia, test_loader_pneumonia, device, convert_to_binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transfer learning lets go back to the full dataset\n",
    "\n",
    "model_chest_full = Net(in_channels=n_channels_chest, num_classes=2).to(device)\n",
    "optimizer_chest_full = optim.SGD(model_chest_full.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# train\n",
    "model_chest_full = train(model_chest_full, train_loader_chest_full, optimizer_chest_full, convert_to_binary=True, epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning - Frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pneumonia_transfer_frozen = Net(in_channels=n_channels_pneumonia, num_classes=2).to(device)\n",
    "\n",
    "# TODO: Transfer the weights of the ENCODER from the chest model to the pneumonia model\n",
    "\n",
    "# TODO: Freeze all layers except the FC\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer_pneumonia_transfer_frozen = optim.SGD(model_pneumonia_transfer_frozen.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model_pneumonia_transfer_frozen = train(model_pneumonia_transfer_frozen, train_loader_pneumonia, optimizer_pneumonia_transfer_frozen, convert_to_binary=False, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "print('==> Evaluating ...')\n",
    "print('train')\n",
    "test(model_pneumonia_transfer_frozen, train_loader_pneumonia, device, convert_to_binary=False)\n",
    "print('test')\n",
    "test(model_pneumonia_transfer_frozen, test_loader_pneumonia, device, convert_to_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: EXAMINE LATENT SPACE\n",
    "test_embeddings, test_labels = extract_embeddings(test_loader_pneumonia, model_pneumonia_transfer_frozen)\n",
    "\n",
    "# Plot embeddings\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_embeddings and train_labels are obtained as shown in the previous step\n",
    "\n",
    "# Flatten the embeddings\n",
    "test_embeddings_flat = test_embeddings.view(test_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "# TODO: Apply PCA\n",
    "\n",
    "# TODO: Apply t-SNE\n",
    "\n",
    "# Plot PCA result\n",
    "plot_embeddings(pca_result, test_labels.cpu(), 'PCA of Train Embeddings')\n",
    "\n",
    "# Plot t-SNE result\n",
    "plot_embeddings(tsne_result, test_labels.cpu(), 't-SNE of Train Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning - Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pneumonia_transfer_trainable = Net(in_channels=n_channels_pneumonia, num_classes=2).to(device)\n",
    "\n",
    "# TODO: Transfer the weights from the chest model to the pneumonia model\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer_pneumonia_transfer_trainable = optim.SGD(model_pneumonia_transfer_trainable.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model_pneumonia_transfer_trainable = train(model_pneumonia_transfer_trainable, train_loader_pneumonia, optimizer_pneumonia_transfer_trainable, convert_to_binary=False, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "print('==> Evaluating ...')\n",
    "print('train')\n",
    "test(model_pneumonia_transfer_trainable, train_loader_pneumonia, device, convert_to_binary=False)\n",
    "print('test')\n",
    "test(model_pneumonia_transfer_trainable, test_loader_pneumonia, device, convert_to_binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: EXAMINE LATENT SPACE\n",
    "test_embeddings, test_labels = extract_embeddings(test_loader_pneumonia, model_pneumonia_transfer_trainable)\n",
    "\n",
    "# Plot embeddings\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_embeddings and train_labels are obtained as shown in the previous step\n",
    "\n",
    "# Flatten the embeddings\n",
    "test_embeddings_flat = test_embeddings.view(test_embeddings.size(0), -1).cpu().numpy()\n",
    "\n",
    "# TODO: Apply PCA\n",
    "\n",
    "# TODO: Apply t-SNE\n",
    "\n",
    "# Plot PCA result\n",
    "plot_embeddings(pca_result, test_labels.cpu(), 'PCA of Train Embeddings')\n",
    "\n",
    "# Plot t-SNE result\n",
    "plot_embeddings(tsne_result, test_labels.cpu(), 't-SNE of Train Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Transfer learning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
